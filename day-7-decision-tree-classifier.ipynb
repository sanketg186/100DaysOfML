{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Decision Tree Classifier\nIn this notebook we will implement decision tree classifier from scratch","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import KFold\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"class Node():\n    def __init__(self, gini =0, num_samples=0, num_samples_per_class=None, predicted_class=None):\n        self.gini = gini\n        self.entropy  = 0\n        self.left = None\n        self.right = None\n        self. predicted_class = predicted_class \n        self.num_sample_per_class = num_samples_per_class\n        self.feature_index = 0\n        self.threshold = 0","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class DecisionTreeClassifier():\n    def __init__(self,max_depth=20):\n        self.max_depth = max_depth\n    \n    def fit(self,X,y):\n        self.n_features = X.shape[1]\n        self.num_classes = len(set(y))\n        self.tree_root = self.grow_tree(X,y)\n\n    \n    def best_split(self,X,y):\n        best_gain = 0.0\n        best_feature_index = -1\n        best_threshold = 0.0\n        m = y.size\n        if m<=1:\n            return None,None,None\n        \n        for idx in range(self.n_features):\n            \n            feature = X[:,idx]\n            thresholds = list(set(feature))\n            \n            for threshold in thresholds:\n                gain = self.compute_gain(feature,y,threshold)\n                if gain>0 and gain > best_gain:\n                    best_gain = gain\n                    best_feature_index = idx\n                    best_threshold = threshold\n                    \n        return best_gain,best_feature_index,best_threshold\n                    \n        \n    def grow_tree(self,X,y,depth=0):\n        num_samples_per_class = [np.sum(y==i) for i in range(0,self.num_classes)]\n        predicted_class = np.argmax(num_samples_per_class)\n        gini = self.gini_impurity(y)\n        node = Node(\n            gini=gini,\n            num_samples=y.size,\n            num_samples_per_class=num_samples_per_class,\n            predicted_class=predicted_class,\n        )\n        if depth < self.max_depth:\n            best_gain,idx, thr = self.best_split(X, y)\n            if idx is not None:\n                indices_left = X[:, idx] <= thr\n                X_left, y_left = X[indices_left], y[indices_left]\n                X_right, y_right = X[~indices_left], y[~indices_left]\n                node.feature_index = idx\n                node.threshold = thr\n                node.left = self.grow_tree(X_left, y_left, depth + 1)\n                node.right = self.grow_tree(X_right, y_right, depth + 1)\n\n        return node\n        \n        \n    def gini_impurity(self,y):\n        m = y.size\n        p = sum(((np.sum(y==c)/m)**2) for c in range(self.num_classes))\n        gini = 1.0 - p\n        return gini\n    \n    \n    def compute_gain(self,X,y,threshold):\n        \n        parent_gini = self.gini_impurity(y)\n        \n        left_y = y[X<=threshold]\n        right_y = y[X>threshold]\n        \n        left_gini = self.gini_impurity(left_y)\n        right_gini = self.gini_impurity(right_y)\n        \n        gain = parent_gini - (left_gini + right_gini)\n        return gain\n    \n    \n    def predict(self,X):\n        \n        predict_y = []\n        for sample in X:\n            current_node = self.tree_root\n            while current_node.left:\n                if sample[current_node.feature_index]<current_node.threshold:\n                    current_node = current_node.left\n                else:\n                    current_node = current_node.right\n            predict_y.append(current_node.predicted_class)\n        return predict_y","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"np.random.seed(21)\niris = load_iris()\nx = iris.data\ny = iris.target\n\ndecision_tree = DecisionTreeClassifier()\nkf = KFold(n_splits=6, shuffle=True)\n\nfor i, (train, test) in enumerate(kf.split(x)):\n    decision_tree.fit(x[train], y[train])\n    accuracy = (decision_tree.predict(x[test]) == y[test]).sum() / len(y[test])\n    print('{0}-validation accuracy: {1}'.format((i+1), accuracy))","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:60: RuntimeWarning: invalid value encountered in long_scalars\n","output_type":"stream"},{"name":"stdout","text":"1-validation accuracy: 0.84\n2-validation accuracy: 0.96\n3-validation accuracy: 0.88\n4-validation accuracy: 0.92\n5-validation accuracy: 1.0\n6-validation accuracy: 0.96\n","output_type":"stream"}]}]}